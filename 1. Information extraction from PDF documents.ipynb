{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Information extraction from PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction from PDF documents. There are 4 PDF documents in the zip file which are stock contracts used in banks. You need to build an NLP based IE system which can extract the following information from the document. There is a subsection called 'Rounding' in 'Paragraph 11: Elections and Variables' section. You need to extract\n",
    "Currency Amount Rounding (up / down/ nearest) If not mentioned, take it as 'nearest' by default.\n",
    "\n",
    "for 'Delivery Amount' and 'Return Amount'.\n",
    "\n",
    "Eg: In the file '16017sec.pdf' here are the values.\n",
    "\n",
    "Delivery Amount: Currency: USD Amount: 10,000 Rounding: up\n",
    "\n",
    "Recall Amount: Currency: USD Amount: 10,000 Rounding: down\n",
    "\n",
    "A few pointers on this problem\n",
    "\n",
    "· Do not process PDF directly. Use some third party tools to convert it to text. · Some strings can be uniquely identified in the text. Use that to narrow down search space. · Look into POS tagging and Named Entity Recognition. These NLP techniques which will be helpful to crack this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions on execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow to execute the below code:\n",
    "1. Make sure all the required packages mentioned in the \"requirements.txt\" file are installed\n",
    "2. Place the unzipped pdf documents in a folder\n",
    "3. Configure the \"base_path\" in the \"conf.json\" to the path to folder in which pdfs are placed\n",
    "4. Make sure the \"conf.json\" file is always in the same location as this ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the below code for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16017sec.pdf\n",
      "===============================================================\n",
      "The Delivery Amount\n",
      "will be rounded : up\n",
      "currency : USD\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "the Return Amount\n",
      "will be rounded : down\n",
      "currency : USD\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "\n",
      "20197sec.pdf\n",
      "===============================================================\n",
      "The Delivery Amount\n",
      "will be rounded : up\n",
      "currency : EUR\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "the Return Amount\n",
      "will be rounded : down\n",
      "currency : EUR\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "\n",
      "English law VM CSA EXECUTED.PDF\n",
      "===============================================================\n",
      "the Delivery Amount\n",
      "will be rounded : up\n",
      "currency : USD\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "the Return Amount\n",
      "will be rounded : down\n",
      "currency : USD\n",
      "amount : 10,000\n",
      "---------------------------\n",
      "\n",
      "HBUS00002134-00002150.pdf\n",
      "===============================================================\n",
      "The Delivery Amount\n",
      "shall be rounded : up\n",
      "currency : USD\n",
      "amount :lOO , OOO\n",
      "---------------------------\n",
      "the Return Amount\n",
      "shall be rounded : down\n",
      "currency : USD\n",
      "amount :lOO , OOO\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing required packages\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tika import parser\n",
    "\n",
    "# fetching_path_to_documents_location_from_configuration_file\n",
    "with open(\"conf.json\") as json_conf : \n",
    "    CONF = json.load(json_conf)\n",
    "\n",
    "directory = format(CONF[\"base_path\"])\n",
    "#print(format(CONF[\"base_path\"]))\n",
    "    \n",
    "# function to extract grammer pattern from text\n",
    "def grammerPatternExtraction(chunkName):\n",
    "    attribute_list = []\n",
    "    # for each phrase sub tree in the parse tree\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == chunkName):\n",
    "        # appending the phrase from a list of part-of-speech tagged words\n",
    "        attribute_list.append(\" \".join([word for word, pos in subtree]))\n",
    "    return attribute_list\n",
    "\n",
    "# traversing through all the docs in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # filtering the docs ending with .pdf's and .PDF's\n",
    "    if filename.endswith(\".pdf\") or filename.endswith(\".PDF\"): \n",
    "        # building the path to each file\n",
    "        file = Path(directory) / filename\n",
    "        print(filename)\n",
    "        #print(file)\n",
    "        print(\"===============================================================\")\n",
    "        # using tika's parser to extract text from pdf\n",
    "        text = parser.from_file(str(file))\n",
    "        # extracting required content from tika's output\n",
    "        text=text[\"content\"]\n",
    "        text = text.replace(\"\\n\",\"\")\n",
    "        #print(text)\n",
    "        # extracting the part of text lying between \"Rounding\" and \"Valuation and Timing\" keywords\n",
    "        result = re.search(\"%s(.*)%s\" % (\"Rounding\", \"Valuation and Timing\"), text).group(1)\n",
    "        # sentences tokenizer\n",
    "        sentences = nltk.sent_tokenize(str(result))\n",
    "        # word tokenize each sentence\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        # pos tagging each sentence\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "        #print(sentences)\n",
    "        # defining a grammar with a regular-expression rule \n",
    "        grammar = r\"\"\"\n",
    "        NP: {<DT><NNP><NNP>} \n",
    "        VP: {<MD><VB><VBN>} \n",
    "        PP: {<R.*>}\n",
    "        ND: {<NNP><CD>}\n",
    "            {<NNP><,><NNP>}\n",
    "        \"\"\"\n",
    "        # creating a chunk parser\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        # looping over all the sentences from extracted text\n",
    "        for sent in sentences:\n",
    "            tree = cp.parse(sent)\n",
    "            amount_type=[]\n",
    "            relation=[]\n",
    "            indicator=[]\n",
    "            amount=[]\n",
    "            currency_pattern = \"USD|INR|EUR|GBP\"\n",
    "            # calling grammerPatternExtraction method to extract the words which has required chunk grammer patterns\n",
    "            amount_type = grammerPatternExtraction(\"NP\")\n",
    "            relation = grammerPatternExtraction(\"VP\")\n",
    "            indicator = grammerPatternExtraction(\"PP\")\n",
    "            amount = grammerPatternExtraction(\"ND\")\n",
    "            #extracting required keywords and relations from list of extracted words in grammerPatternExtraction\n",
    "            if len(amount_type)>0:\n",
    "                print(amount_type[0])\n",
    "            # skipping the sentences which does't have required grammer patterns\n",
    "            else:\n",
    "                continue\n",
    "            if len(relation)>0 and len(indicator)>0:\n",
    "                print(relation[0]+\" : \"+indicator[0])\n",
    "            # skipping the sentences which does't have required grammer patterns\n",
    "            else:\n",
    "                continue\n",
    "            if len(amount)>0:\n",
    "                currency = \"\".join(re.findall(currency_pattern,amount[0]))\n",
    "                print(\"currency : \"+currency)\n",
    "                print(\"amount :\"+amount[0].replace(currency,\"\"))\n",
    "            # skipping the sentences which does't have required grammer patterns\n",
    "            else:\n",
    "                continue\n",
    "            print(\"---------------------------\")\n",
    "            if len(amount_type)>1:\n",
    "                print(amount_type[1])\n",
    "            if len(relation)>0:\n",
    "                if len(indicator)>1:\n",
    "                    print(relation[0]+\" : \"+indicator[1])\n",
    "                elif len(indicator)>0:\n",
    "                    print(indicator[0]+\" : \"+indicator[0])\n",
    "            # separating currancy and amount from extracted amount\n",
    "            if len(amount)>1:\n",
    "                currency = \"\".join(re.findall(currency_pattern,amount[1]))\n",
    "                print(\"currency : \"+currency)\n",
    "                print(\"amount :\"+amount[1].replace(currency,\"\"))\n",
    "            elif len(amount)>0:\n",
    "                currency = \"\".join(re.findall(currency_pattern,amount[0]))\n",
    "                print(\"currency : \"+currency)\n",
    "                print(\"amount :\"+amount[0].replace(currency,\"\"))\n",
    "            #print(tree)\n",
    "            #result.draw()\n",
    "            print(\"---------------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
